---
title: "IE582 HW_3"
author: "Abdulsamed Kağıt"
date: "27/12/2020"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```

## Task A 

Assume that you are willing to use 168 and 48 hours ago consumption values as your naive approaches to predict next day’s consumption. Suppose the test
period includes the dates after 1st of November, 2020 (included). For both approaches, report the summary statistics of MAPE values for the test period.

```{r Data Reading, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
#Libraries
library(rmarkdown)
library(ggplot2)
library(dplyr)
library(yardstick)
library(broom)
library(purrr)
library(data.table)
library(stringr)
library(tidyr)
library(ggfortify)
library(tidyverse)
library(Metrics)
library(glmnet)
library(CVXR)
library(plotly)
library(viridis)

#Data Reading
set.seed(1234)
consumption <- fread("C:/Users/Abdulsamed/Documents/GitHub/fall20-samedkagit/files/HW3/RealTimeConsumption-01012016-01122020.csv")
consumption$Tarih <- paste(consumption$Tarih,consumption$Saat)
consumption <-consumption %>%select(-Saat)

consumption$Tarih <- as.POSIXct(consumption$Tarih, format = '%d.%m.%Y %H:%M', tz="Europe/Istanbul")
consumption <- consumption %>% rename(Tuketim=`Tüketim Miktarı (MWh)`)
consumption$Tuketim <- gsub("\\.", "",consumption$Tuketim)
consumption$Tuketim <- gsub("\\,", ".",consumption$Tuketim)
consumption$Tuketim <- as.numeric(consumption$Tuketim)
consumption <- consumption %>% mutate(lag_168 = lag(Tuketim,168), lag_48 = lag(Tuketim,48)) %>% drop_na()

duplicated = consumption[duplicated(consumption$Tarih)]$Tarih
consumption <- consumption %>% filter(!(as.IDate(Tarih) %in% c(as.IDate(duplicated),as.IDate(duplicated+3600*24*2),as.IDate(duplicated+3600*24*7))))

#TASK A

naive_test <- consumption %>% filter(Tarih >= as.POSIXct("01-11-2020", format = '%d-%m-%Y'))

mape_lag48_hourly <- naive_test %>% group_by(as.ITime(Tarih)) %>% summarize(mape= mape(Tuketim,lag_48)) %>% rename(Saat = `as.ITime(Tarih)`)
mape_lag168_hourly <- naive_test %>% group_by(as.ITime(Tarih)) %>% summarize(mape= mape(Tuketim,lag_168)) %>% rename(Saat = `as.ITime(Tarih)`)

paged_table(mape_lag48_hourly)
paged_table(mape_lag168_hourly)

```

MAPE values calculated by utilizing 1 week ago consumption values are considerably lower than that of calculated by utilizing 2 days ago consumption. 
Accordingly,it can be said that electricity consumption data have weekly seasonality. (The day of the week matters)

## Task B

Instead of using the lag consumptions in part (A) as a forecast, we would like to treat them as our features and build a linear regression model. Train your
model using the data till 1st of November, 2020 and test on the rest. Your linear regression model is expected to include aforementioned two features (i.e. 
Lag_48 and  Lag_168) and an intercept. Report the summary statistics of MAPE values for the test period.

```{r Linear Model, echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}
#TASK B

lm_train <- consumption %>% filter(Tarih < as.POSIXct("01-11-2020", format = '%d-%m-%Y'))
lm_test <-  consumption %>% filter(Tarih >= as.POSIXct("01-11-2020", format = '%d-%m-%Y'))

lm_model <- lm(Tuketim ~ lag_48+lag_168, lm_train)

lm_test$prediction <- predict(lm_model,lm_test)
lm_mape <- lm_test %>% group_by(as.ITime(Tarih)) %>% summarize(mape= mape(Tuketim, prediction)) %>% rename(Saat = `as.ITime(Tarih)`)
paged_table(lm_mape)
```


## Task C

As mentioned earlier, hourly seasonality is important. Although we used the same hour’s consumption value of the past days to handle this problem for part 
(B), we implicitly impose an assumption that prediction model for each hour has the same coefficients which may not be correct since the consumption behavior
at nights can be different than the other hours. Therefore, modeling each hour separately is another way to approach to the same problem. Train linear 
regression models for each hour using the same training period (24 models) and report your test performance as in part (A).

```{r Hourly Linear Model, message=FALSE, warning=FALSE, paged.print=TRUE}
#TASK C

lm_train_hourly <- lm_train %>% mutate(saat = as.ITime(Tarih))
lm_test_hourly  <- lm_test %>% mutate(saat = as.ITime(Tarih))  %>% select(-prediction)

lm_models_hourly<-lm_train_hourly%>%group_by(saat)%>%nest()%>% 
  mutate(fit=purrr::map(data,function(d)lm(Tuketim~lag_168+lag_48,data=d)))

lm_models_hourly_mape = data.frame( hour = 0:23 , mape = 0)
hour_0 = as.ITime(as.POSIXct("01-11-2020",format ='%d-%m-%Y'))

for( i in 1:24)
{
predict_data <- lm_test_hourly %>% filter( saat==hour_0+3600*(i-1) )
lm_models_hourly_mape$mape[i]<-mape(predict_data$Tuketim,predict(lm_models_hourly$fit[[i]],predict_data))
rm(predict_data)
}

paged_table(lm_models_hourly_mape)

print(paste("For how many hours, MAPE value calculated by utilizing linear models developed for each hour seperately are greater than that of calculated by utilizing linear model developed for the whole train data:",sum((lm_models_hourly_mape$mape - lm_mape$mape)>0)))

```

Modeling each hour separately does not result in better in terms of average MAPEs but the variance of MAPEs seem to be lower than previous approach.

## Task D

One of your friends comes up with an alternative approach assuming that all hourly consumption values of last week (same day) can be important in the 
prediction of the next day’s consumption. In other words, you can use the 24 consumption values of the last week to predict next day’s consumption. Assume 
that you have 48 features (hourly consumption from two days ago and last week’s hourly consumption) in total. You are also willing to follow the same logic 
in part (C) and build a prediction model for each hour separately. Since there is a strong correlation (actually an autocorrelation) between these 
predictors, you are willing to use penalized regression approaches for modeling. Use L1 penalty in your regression models for each hour. Note that the 
feature matrix will be the same for all your models, only the target variable will change for this task. In order to determine the regularization parameter 
(i.e. lambda), perform a 10-fold cross-validation. Train penalized regression models with L1 penalty (i.e. lasso regression) for each hour using the same 
training period (24 models) and report your test performance as in part (A). Also comment on the resulting models (i.e. coefficients and etc.).


```{r Lasso Model, message=FALSE, warning=FALSE, paged.print=TRUE}
#TASK D

consumption2 <- fread("C:/Users/Abdulsamed/Documents/GitHub/fall20-samedkagit/files/HW3/RealTimeConsumption-01012016-01122020.csv")

consumption2$Tarih <- as.POSIXct(consumption2$Tarih, format = '%d.%m.%Y', tz="Europe/Istanbul")
consumption2 <- consumption2 %>% rename(Tuketim=`Tüketim Miktarı (MWh)`)
consumption2$Tuketim <- gsub("\\.", "",consumption2$Tuketim)
consumption2$Tuketim <- gsub("\\,", ".",consumption2$Tuketim)
consumption2$Tuketim <- as.numeric(consumption2$Tuketim)
consumption2 <- consumption2 %>% filter(!(as.IDate(Tarih) %in% c(as.IDate(duplicated))))



consumption_lag_48 <- consumption2 %>% pivot_wider(names_from = Saat, values_from= Tuketim, names_prefix="lag_48_")
consumption_lag_168 <- consumption2 %>% pivot_wider(names_from = Saat, values_from= Tuketim, names_prefix="lag_168_")

consumption_lag_48$Tarih = consumption_lag_48$Tarih + 3600*48
consumption_lag_168$Tarih = consumption_lag_168$Tarih + 3600*168

lasso_data <-consumption2 %>% inner_join(consumption_lag_48, by = "Tarih")
lasso_data <- lasso_data %>% inner_join(consumption_lag_168, by="Tarih")


lasso_data_train <- lasso_data %>% filter(Tarih < as.POSIXct("01-11-2020", format = '%d-%m-%Y'))
lasso_data_test <-  lasso_data %>% filter(Tarih >= as.POSIXct("01-11-2020", format = '%d-%m-%Y'))

set.seed(1234)
lasso_models_hourly<-lasso_data_train%>%group_by(Saat)%>%nest()%>% 
  mutate(fit=purrr::map(data,function(d)cv.glmnet(data.matrix(d%>%select(contains("lag"))),data.matrix(d$Tuketim),family="gaussian",standardize =TRUE)))
      

lasso_models_hourly_mape = data.frame( hour = 0:23 , mape = 0)
k=1

for( i in c("00:00","01:00","02:00","03:00","04:00","05:00","06:00","07:00","08:00","09:00",paste(10:23,":00",sep="")))
{
  predict_data <- lasso_data_test %>% filter(Saat == i)
  lasso_models_hourly_mape$mape[k]<-mape(predict_data$Tuketim,predict(lasso_models_hourly$fit[[k]],data.matrix(predict_data%>%select(contains("lag"))),s= "lambda.1se"))
  k = k+1
  rm(predict_data)
}
rm(k)
paged_table(lasso_models_hourly_mape)
coefs <- matrix(0,nrow = 49 ,ncol = 1)

for(i in 1:24)
{
  coefs = bind_cols(coefs,as.matrix(coef(lasso_models_hourly$fit[[i]],s="lambda.1se")))
}
coefs <- coefs[,-1]
rownames(coefs) = rownames(coef(lasso_models_hourly$fit[[1]],s="lambda.1se"))

used_coefs <- rowSums(coefs > 0)
as.matrix(used_coefs,ncol=1)
```

MAPE values drastically decreased compared to previous approaches. Hours from 10:00 to 15:00 are the ones with the highest MAPE values for all models.
From the table above, most frequently used features in models can be seen. Intercept is common for each 24 model. Consumption value of hour 5-6-7-13-20-23
from 2 days ago and consumption value of hour 7-8-16 from 1 week ago are frequently used. It is remarkable that consumption value of hour 13 and 23 from 2
days ago and hour 7 from 1 week ago is used in all of the models.

## Task E

An alternative approach to lasso regression in part (D) is to perform the same tasks using fused penalties since the predictors are basically time series. 
While building a model to predict an hour’s consumption, we explicitly used two time series (Lag_Day_7 and Lag_Day_2 hourly consumption values). Therefore, 
penalizing the coefficients for consecutive predictors of each time series (Lag 7 and Lag 2) may provide in performance improvements. Following a similar 
logic as in part (d), train regression models with fused penalties for predicting each hour. You can use a single penalty term for the fused penalties (i.e. 
lambda_1) and L1 penalty for coefficients (i.e. lambda_2). In order to tune your model, you can perform 10-fold cross-validation.

```{r Fused Lasso Model, message=FALSE, warning=FALSE, paged.print=TRUE}
#TASK E
lasso_data_model <- lasso_data
fused_lasso_results = data.table(hour="00:00",lambda=0, objective = 0,MAPE = 0)
thresh <- 1e-7
invisible({capture.output({
for( j in c("00:00","01:00","02:00","03:00","04:00","05:00","06:00","07:00","08:00","09:00",paste(10:23,":00",sep="")))
{
for(lambda in c(100000000,50000000,20000000,seq(200000,1100000,300000)))
{
for(i in 1:10)
{
hour_data<-lasso_data_model %>%filter(Saat == j)%>%filter(Tarih < as.POSIXct("01-11-2020",format ='%d-%m-%Y'))
test_data <- hour_data[(round(nrow(hour_data)*0.1)*(i-1)+1):(round(nrow(hour_data)*0.1)*i),]  

train_data <- anti_join(hour_data,test_data,by=c("Tarih","Saat")) %>% select(matches("lag_|Tuketim"))
test_data <- test_data %>% select(matches("lag_|Tuketim"))
betaHat <- Variable(49) # betaHat[49] is the intercept
matrix1 <- as.matrix(train_data%>%select(-Tuketim))
matrix2 <- as.matrix(test_data%>%select(-Tuketim))

objective<-(sum((train_data$Tuketim-((matrix1%*%betaHat[1:48])+betaHat[49]))^2)+lambda*sum(betaHat[1:48]^2)+lambda*sum(abs(betaHat[2:24]-betaHat[1:23]))+lambda*sum(abs(betaHat[26:48]-betaHat[25:47])))/nrow(train_data)

problem <- Problem(Minimize(objective))
result <- solve(problem, FEASTOL = thresh, RELTOL = thresh, ABSTOL = thresh, verbose = TRUE)
beta <- result$getValue(betaHat)

temp<-data.table(hour=j,lambda=lambda, objective= result$value , MAPE = mape(test_data$Tuketim,(matrix2%*%beta[1:48])+beta[49]))
fused_lasso_results <- rbind(fused_lasso_results,temp)
}
}
}
})})

evaluation <- fused_lasso_results[-1,] %>%group_by(hour,lambda) %>% 
  summarise(Mean_MAPE = mean(MAPE), Se_MAPE = sqrt(var(MAPE)), Mean_Objective = mean(objective))%>%ungroup()%>% arrange(hour,lambda)
paged_table(evaluation)
```


10-fold cross validation is performed for each alternative lambda on train data. It can be seen that Lambda = 20000000 gives the best mean MAPE among all 
other lambda values for each hour. This value will be used to train fused penalized model and then the model will be tested on test data. Note that beta[49] 
is the intercept in this model, hence it does not show in the fused Lasso penalty or ridge penalty. Also note that beta[25] - beta[24] is not included to
fused penalty since they belong to different time series. (Lag 48 and Lag 168)


```{r Fused Lasso Model with the best lambda, error=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
# Lambda = 20000000

fused_lasso_results2 = data.table(hour="00:00",lambda=0, objective = 0,MAPE = 0)
betas = paste("b",1:49,sep = "")
fused_lasso_results2[,betas] <- 0
invisible({capture.output({
for( j in c("00:00","01:00","02:00","03:00","04:00","05:00","06:00","07:00","08:00","09:00",paste(10:23,":00",sep="")))
{
  for(lambda in c(20000000))
  {
      hour_data<-lasso_data_model %>%filter(Saat == j)%>%filter(Tarih < as.POSIXct("01-11-2020",format ='%d-%m-%Y'))%>%select(matches("lag_|Tuketim"))
      test_data<-lasso_data_model %>%filter(Saat == j)%>%filter(Tarih >= as.POSIXct("01-11-2020",format ='%d-%m-%Y'))%>%select(matches("lag_|Tuketim"))
      
      betaHat <- Variable(49) # betaHat[49] is the intercept
      matrix1 <- as.matrix(hour_data%>%select(-Tuketim))
      matrix2 <- as.matrix(test_data%>%select(-Tuketim))
      
      objective<-sum((hour_data$Tuketim-((matrix1%*%betaHat[1:48])+betaHat[49]))^2)+lambda*sum(betaHat[1:48]^2)+lambda*sum(abs(betaHat[2:24]-betaHat[1:23]))+lambda*sum(abs(betaHat[26:48]-betaHat[25:47]))
      
      problem <- Problem(Minimize(objective))
      result <- solve(problem, FEASTOL = thresh, RELTOL = thresh, ABSTOL = thresh, verbose = TRUE)
      beta <- result$getValue(betaHat)
      
      temp<-data.table(hour=j,lambda=lambda,objective= result$value, MAPE = mape(test_data$Tuketim,(matrix2%*%beta[1:48])+beta[49]))
      for(s in 1:49)
      temp[,betas[s]] <- beta[s]
      fused_lasso_results2 <- rbind(fused_lasso_results2,temp)
  }
}
})})
fused_lasso_results2 <- fused_lasso_results2[-1,]
paged_table(fused_lasso_results2)
```

Model coefficients and MAPE values can be seen from the table above. MAPE values are slightly decreased for almost each hour compared to lasso regression.
Note that all coefficients (i.e. beta1, beta2..) are non-zero in this model.

## Task F

Compare the results drawing a box-plot of MAPE values for each approach on same plot. Comment on your findings.

```{r Boxplot of MAPEs, message=FALSE, warning=FALSE}
#Task F

data1 <- fused_lasso_results2 %>% select(hour,MAPE)
data1[,"name"] <- "Fused_Lasso"

data2 <- lasso_models_hourly_mape %>% rename(MAPE = mape)
data2[,"name"] <- "Hourly_Lasso"
data2$hour <- data1$hour

data3 <- lm_mape %>% rename(hour =Saat, MAPE = mape)
data3[,"name"] <- "Linear_Model"
data3$hour <- data1$hour

data4 <- mape_lag168_hourly %>% rename(hour =Saat, MAPE = mape)
data4[,"name"] <- "Lag_168"
data4$hour <- data1$hour

data5 <- lm_models_hourly_mape %>% rename(MAPE = mape)
data5$hour <- data1$hour
data5[,"name"] <-"Hourly_Linear"

data6 <- mape_lag48_hourly %>% rename(hour =Saat, MAPE = mape)
data6[,"name"] <- "Lag_48"
data6$hour <- data1$hour

data <- rbind(data1,data2,data3,data4,data5,data6)
rm(data1,data2,data3,data4,data5,data6)
# Plot
data %>% ggplot( aes(x=name, y=MAPE, fill=name)) + geom_boxplot() +theme(legend.position="none",plot.title = element_text(size=11)) +
  ggtitle("MAPE Comparison") + xlab("Methods")

```

Fused lasso approach and separate lasso model for each hour approach yields the lowest MAPE values within narrow quantiles. Consumption values from 1 week
ago is the third best approach which is followed by the linear regression for each hours approach. Consumption values from 1 week ago have the lowest 
variability compared to all other approaches. MAPE values of linear regression approach have relatively large variability but the same mean with linear
regression for each hour approach. Consumption values from 2 days ago is the worst approach which has the highest mean and variance.
